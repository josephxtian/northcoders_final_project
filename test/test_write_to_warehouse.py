import pytest
import pandas as pd
import boto3
from unittest.mock import patch, MagicMock
from io import StringIO
from moto import mock_aws
from src.write_to_warehouse import read_from_s3_processed_bucket, write_to_warehouse
import os
import io
from datetime import datetime, timedelta
import psycopg2
import botocore.exceptions


# @pytest.fixture
# def mock_db_connection():
#     with patch("psycopg2.connect") as mock_connect:
#         mock_conn = MagicMock()
#         mock_cur = MagicMock()
#         mock_connect.return_value = mock_conn
#         mock_connect.return_value = mock_cur
#         yield mock_conn, mock_cur

mock_data = {
    "fact_sales_order": pd.DataFrame({"col1": [1, 2, 3], "col2": ["a", "b", "c"]}),
    "dim_staff": pd.DataFrame({"col1": [4, 5, 6], "col2": ["d", "e", "f"]}),
}

@pytest.fixture
def mock_db_connection():
    # Mock psycopg2.connect to return a mock connection and cursor
    with patch("psycopg2.connect") as mock_connect:
        mock_conn = MagicMock()
        mock_cur = MagicMock()
        # mock_connect.return_value = mock_conn
        mock_conn.cursor.return_value = mock_cur
        return mock_conn, mock_cur

       

class TestReadFromBucket:
# test 1 - reads file from s3 processed bucket
    @mock_aws
    def test_s3_read_from_bucket(self):

        s3_client = boto3.client("s3", region_name="eu-west-2")
        bucket_name = "processed-bucket20250303162226216400000005"
        s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={"LocationConstraint": "eu-west-2"})

        mock_s3 = boto3.client("s3", region_name="eu-west-2")

        fake_parquet_data = io.BytesIO()

        mock_df = pd.DataFrame({"col1": [1, 2, 3], "col2": ["a", "b", "c"]})

        mock_df.to_parquet(fake_parquet_data, engine="pyarrow", index=False)

        fake_parquet_data.seek(0)

        table_name = "fact_sales_order"
        timestamp = "2025-03-06_10-01-45"
        file_key = f"{table_name}/{table_name}_{timestamp}.parquet"
        s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=fake_parquet_data.getvalue())

        output = read_from_s3_processed_bucket(s3_client=s3_client)

        # result_df = pd.read_parquet(io.BytesIO(output["fact_sales_order"]), engine="pyarrow")

        result_df = output["fact_sales_order"]

        pd.testing.assert_frame_equal(result_df, mock_df)


    # test 2 - empty bucket contents returns empty dictionary
    @mock_aws
    def test_for_empty_bucket(self):
        s3_client = boto3.client("s3", region_name="eu-west-2")

        bucket_name = "processed-bucket20250303162226216400000005"

        s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={"LocationConstraint": "eu-west-2"})

        output = read_from_s3_processed_bucket(s3_client=s3_client)

        assert output == {}


    # test 3 - function reads and extracts most recent file
    @mock_aws
    def test_function_extracts_most_recent_file(self):
        s3_client = boto3.client("s3", region_name="eu-west-2")
        bucket_name = "processed-bucket20250303162226216400000005"
        s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={"LocationConstraint": "eu-west-2"})

        table_name = "fact_sales_order"
        
        # Create two timestamps, one older and one newer
        old_timestamp = (datetime.utcnow() - timedelta(days=1)).strftime("%Y-%m-%d_%H-%M-%S")
        new_timestamp = datetime.utcnow().strftime("%Y-%m-%d_%H-%M-%S")

        old_file_key = f"{table_name}/{table_name}_{old_timestamp}.parquet"
        new_file_key = f"{table_name}/{table_name}_{new_timestamp}.parquet"

        # Create fake Parquet data
        mock_df_old = pd.DataFrame({"col1": [1, 2, 3], "col2": ["x", "y", "z"]})
        mock_df_new = pd.DataFrame({"col1": [4, 5, 6], "col2": ["a", "b", "c"]})

        fake_parquet_old = io.BytesIO()
        fake_parquet_new = io.BytesIO()
        mock_df_old.to_parquet(fake_parquet_old, engine="pyarrow", index=False)
        mock_df_new.to_parquet(fake_parquet_new, engine="pyarrow", index=False)
        
        fake_parquet_old.seek(0)
        fake_parquet_new.seek(0)

        # Upload both files to mock bucket
        s3_client.put_object(Bucket=bucket_name, Key=old_file_key, Body=fake_parquet_old.getvalue())
        s3_client.put_object(Bucket=bucket_name, Key=new_file_key, Body=fake_parquet_new.getvalue())

        output = read_from_s3_processed_bucket(s3_client=s3_client)

        # Checks if the latest file was selected
        result_df = output["fact_sales_order"]
        pd.testing.assert_frame_equal(result_df, mock_df_new)

    # test 4 - function attempts to read a corrupted parquet
    @mock_aws
    def test_function_for_corrupted_parquet(self):
        s3_client = boto3.client("s3", region_name="eu-west-2")
        bucket_name = "processed-bucket20250303162226216400000005"
        s3_client.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={"LocationConstraint": "eu-west-2"})

        table_name = "fact_sales_order"
        timestamp = "2025-03-06_10-01-45"
        file_key = f"{table_name}/{table_name}_{timestamp}.parquet"

        # Upload a corrupted "Parquet" file (invalid binary data)
        s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=b"this is not a parquet file")

        # Call the function and expect False for try and exception raised
        try:
            read_from_s3_processed_bucket(s3_client=s3_client)
            assert False, "Expected an exception for a corrupt Parquet file"
        except Exception as e:
            assert "Parquet magic bytes not found" in str(e), f"Unexpected exception: {e}"


    # test 5 - function attempts to read all tables even if some aren't available
    @mock_aws
    def test_function_reads_all_tables(self):
        s3_client = boto3.client("s3", region_name="eu-west-2")
        bucket_name = "processed-bucket20250303162226216400000005"
        try:
            s3_client.create_bucket(
            Bucket=bucket_name,
            CreateBucketConfiguration={"LocationConstraint": "eu-west-2"},
        )
        except botocore.exceptions.ClientError as e:
            error_code = e.response["Error"]["Code"]
            if error_code != "BucketAlreadyOwnedByYou":
                raise  # Re-raise if it's an unexpected error

        # Only uploading a file for fact_sales_order (missing other tables)
        table_name = "fact_sales_order"
        timestamp = "2025-03-06_10-01-45"
        file_key = f"{table_name}/{table_name}_{timestamp}.parquet"

        mock_df = pd.DataFrame({"col1": [1, 2, 3], "col2": ["a", "b", "c"]})
        fake_parquet = io.BytesIO()
        mock_df.to_parquet(fake_parquet, engine="pyarrow", index=False)
        fake_parquet.seek(0)

        s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=fake_parquet.getvalue())

        output = read_from_s3_processed_bucket(s3_client=s3_client)

        # Ensure fact_sales_order is in the output
        assert "fact_sales_order" in output, "Expected 'fact_sales_order' in output"
        pd.testing.assert_frame_equal(output["fact_sales_order"], mock_df)

        # Ensure other tables are missing
        for table in ["dim_staff", "dim_date", "dim_counterparty", "dim_location"]:
            assert table not in output, f"Expected '{table}' to be missing from output"


class TestWriteToWarehouse:

    @patch("psycopg2.connect")
    # test data schema is successfully written to warehouse tds
    def test_write_to_warehouse_success(self, mock_connect, mock_db_connection):
        # Arrange: Set up the mock cursor and connection
        mock_conn, mock_cur = mock_db_connection
        mock_connect.return_value = mock_conn

        # Simulate what should happen during the database interaction
        mock_cur.fetchall.return_value = [(1, "a"), (2, "b"), (3, "c")]

        mock_data = {"table1": None, "table2": None} 

        # Act: Call the function with the mock data
        write_to_warehouse(mock_data)

        # Assert: Check that the cursor's execute method was called with the correct SQL query
        for table_name in mock_data:
            mock_cur.execute.assert_any_call(f"SELECT * FROM {table_name}")

        # Assert: Verify that fetchall was called after execute (to fetch results)
        mock_cur.fetchall.assert_called()

        # Assert: Verify that the connection and cursor were closed properly
        mock_cur.close.assert_called()
        mock_conn.close.assert_called()

    @patch("psycopg2.connect")
    def test_write_to_warehouse_failure(self, mock_connect):
        mock_connect.side_effect = psycopg2.OperationalError("Connection failed")

        with pytest.raises(RuntimeError, match="Warehouse database operation failed: Connection failed"):
            write_to_warehouse({"table1": None})

    @patch("psycopg2.connect")
    def test_write_to_warehouse_empty_data(self, mock_connect, mock_db_connection):
        mock_conn, mock_cur = mock_db_connection
        mock_connect.return_value = mock_conn

        write_to_warehouse({})

        mock_cur.execute.assert_not_called()
        mock_cur.close.assert_called()
        mock_conn.close.assert_called()




            # mock_boto_client.return_value = mock_s3

            # mock_s3.create_bucket(Bucket="test-bucket", CreateBucketConfiguration={"LocationConstraint": "eu-west-2"})

            # mock_s3.put_object(Bucket="test-bucket", Key="folder/Hello_world.txt", Body="Hello world")

            # response = mock_s3.list_objects_v2(Bucket="test-bucket", Prefix="folder/")

            # mock_s3.get_object(Bucket="test-bucket", Key="folder/Hello_world.txt")

            # print(response)


        # mock_s3.get_object.return_value = {"Body": fake_parquet_data}

        # output = read_from_s3_processed_bucket(s3_client=mock_s3)

        # result_df = pd.read_parquet(io.BytesIO(output["fact_sales_order"]), engine="pyarrow")

        # pd.testing.assert_frame_equal(result_df, mock_df)



        # print(response)

        # mock_s3.list_objects_v2.return_value = {
        # "Contents": [{"Key": "fact_sales_order_2025-03-06_10-01-45.parquet"}]
        # }

        # fake_parquet_data = io.BytesIO()



        # output = read_from_s3_processed_bucket(s3_client=mock_s3)

        # result_df = pd.read_parquet(io.BytesIO(output["fact_sales_order"]), engine="pyarrow")

        # pd.testing.assert_frame_equal(result_df, mock_df)

        # self.assertIn("fact_sales_order", result_df)

        # self.assertIsInstance(result_df["fact_sales_order"], pd.DataFrame)





    # mock_s3 = MagicMock()
    
    # mock_boto_client.return_value = mock_s3
    
    # mock_s3.list_objects_v2.return_value = {"Contents": [{"Key": "fact_sales_order_2025-03-06_10-01-45.parquet"}]}

    # mock_df = pd.DataFrame({"column1": [1, 2, 3]})

    # mock_s3.get_object.return_value = {"Body": io.BytesIO(b"dogandcat")} 
    
    # output = read_from_s3_processed_bucket()
    
    # assert output["fact_sales_order"] == b"dogandcat"

    # mock_s3 = MagicMock()





# test 2 - retrieving data from s3 processed bucket
# test 3 - data successfully populated to warehouse
# test 4 - failure exceptions are captured and printed